import Groq from 'groq-sdk';

const apiKey = process.env.GROQ_API_KEY?.trim();
const GEMINI_API_KEY = process.env.GEMINI_API_KEY?.trim();

let groqClient = null;

const GROQ_MODELS_CASCADE = [
  { id: 'llama-3.3-70b-versatile', name: 'Llama 3.3 70B', priority: 1 },
  { id: 'llama-3.1-8b-instant', name: 'Llama 3.1 8B Fast', priority: 2 },
  { id: 'meta-llama/llama-4-scout-17b-16e-instruct', name: 'Llama 4 Scout 17B', priority: 3 },
  { id: 'qwen/qwen3-32b', name: 'Qwen3 32B', priority: 4 }
];

const GEMINI_MODEL = 'gemini-2.0-flash';

export function getGroqClient() {
  if (!groqClient) {
    if (!apiKey) {
      throw new Error('‚ùå GROQ_API_KEY n√£o encontrada! Verifique a vari√°vel de ambiente.');
    }
    
    // Suporte a chaves Groq (gsk_...) ou OpenRouter (sk-or-...) ou outras compat√≠veis
    if (!apiKey.startsWith('gsk_') && !apiKey.startsWith('sk-')) {
      console.warn('‚ö†Ô∏è GROQ_API_KEY pode ter formato inesperado, tentando mesmo assim...');
    }
    
    groqClient = new Groq({
      apiKey: apiKey
    });
    
    console.log('‚úÖ Conex√£o com Groq: OK');
  }
  return groqClient;
}

function isRateLimitError(error) {
  if (!error) return false;
  
  if (error.status === 429) return true;
  if (error.statusCode === 429) return true;
  if (error.response?.status === 429) return true;
  
  if (error.code === 'rate_limit_exceeded') return true;
  if (error.error?.code === 'rate_limit_exceeded') return true;
  
  const message = error.message || error.error?.message || '';
  if (message.includes('429')) return true;
  if (message.toLowerCase().includes('rate_limit')) return true;
  if (message.toLowerCase().includes('rate limit')) return true;
  if (message.toLowerCase().includes('too many requests')) return true;
  
  return false;
}

function isModelNotFoundError(error) {
  if (!error) return false;
  
  if (error.status === 404) return true;
  if (error.statusCode === 404) return true;
  
  const message = error.message || error.error?.message || '';
  if (message.includes('404')) return true;
  if (message.toLowerCase().includes('not found')) return true;
  if (message.toLowerCase().includes('model not found')) return true;
  
  return false;
}

async function generateWithGemini(messages, options = {}) {
  if (!GEMINI_API_KEY) {
    throw new Error('Gemini n√£o dispon√≠vel - GEMINI_API_KEY n√£o configurada');
  }
  
  console.log(`üîÑ [GROQ-FALLBACK] Usando Gemini (${GEMINI_MODEL}) como fallback final...`);
  
  const systemMessage = messages.find(m => m.role === 'system')?.content || '';
  const userMessage = messages.find(m => m.role === 'user')?.content || '';
  const fullPrompt = systemMessage ? `${systemMessage}\n\n---\n\n${userMessage}` : userMessage;
  
  const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_MODEL}:generateContent?key=${GEMINI_API_KEY}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [
        {
          role: 'user',
          parts: [{ text: fullPrompt }]
        }
      ],
      generationConfig: {
        temperature: options.temperature || 0.7,
        topP: options.top_p || 0.9,
        maxOutputTokens: options.max_tokens || 4000
      }
    })
  });

  if (!response.ok) {
    const errorData = await response.json();
    throw new Error(`Gemini API Error [${response.status}]: ${JSON.stringify(errorData)}`);
  }

  const data = await response.json();
  const content = data?.candidates?.[0]?.content?.parts?.[0]?.text;

  if (!content) {
    throw new Error('Resposta inv√°lida do Gemini - sem conte√∫do');
  }

  console.log(`‚úÖ [GROQ-FALLBACK] Gemini respondeu com sucesso!`);
  
  return {
    choices: [{
      message: { content },
      finish_reason: 'stop'
    }],
    usage: { total_tokens: Math.ceil(content.length / 4) },
    model: GEMINI_MODEL,
    provider: 'gemini'
  };
}

export async function withMultiModelFallback(createRequestFn, options = {}) {
  const {
    maxRetriesPerModel = 2,
    useGeminiFallback = true,
    logPrefix = '[GROQ]'
  } = options;

  const errors = [];
  let lastError = null;
  
  for (const modelConfig of GROQ_MODELS_CASCADE) {
    console.log(`${logPrefix} üîÑ Tentando modelo: ${modelConfig.name} (${modelConfig.id})`);
    
    for (let attempt = 0; attempt < maxRetriesPerModel; attempt++) {
      try {
        const client = getGroqClient();
        const response = await createRequestFn(client, modelConfig.id);
        
        console.log(`${logPrefix} ‚úÖ Sucesso com ${modelConfig.name} na tentativa ${attempt + 1}`);
        
        return {
          ...response,
          _metadata: {
            model: modelConfig.id,
            modelName: modelConfig.name,
            provider: 'groq',
            attempts: attempt + 1,
            totalModelsTriad: GROQ_MODELS_CASCADE.indexOf(modelConfig) + 1,
            usedFallback: modelConfig.priority > 1
          }
        };
      } catch (error) {
        lastError = error;
        const errorInfo = {
          model: modelConfig.id,
          attempt: attempt + 1,
          error: error.message,
          isRateLimit: isRateLimitError(error),
          isModelNotFound: isModelNotFoundError(error)
        };
        errors.push(errorInfo);
        
        console.warn(`${logPrefix} ‚ö†Ô∏è Falha com ${modelConfig.name} (tentativa ${attempt + 1}/${maxRetriesPerModel}):`, error.message);
        
        if (isRateLimitError(error)) {
          console.log(`${logPrefix} üîÑ Rate limit detectado, pulando para pr√≥ximo modelo...`);
          break;
        }
        
        if (isModelNotFoundError(error)) {
          console.log(`${logPrefix} üîÑ Modelo n√£o encontrado, pulando para pr√≥ximo modelo...`);
          break;
        }
        
        if (attempt < maxRetriesPerModel - 1) {
          const waitTime = Math.pow(2, attempt) * 1000;
          console.log(`${logPrefix} ‚è≥ Aguardando ${waitTime}ms antes de tentar novamente...`);
          await new Promise(r => setTimeout(r, waitTime));
        }
      }
    }
  }
  
  if (useGeminiFallback && GEMINI_API_KEY) {
    console.log(`${logPrefix} üîÑ Todos os modelos Groq falharam. Tentando Gemini como fallback final...`);
    
    try {
      const messagesFromRequest = options.messages || [];
      const response = await generateWithGemini(messagesFromRequest, options.generationConfig || {});
      
      return {
        ...response,
        _metadata: {
          model: GEMINI_MODEL,
          modelName: 'Google Gemini',
          provider: 'gemini',
          attempts: 1,
          totalModelsTriad: GROQ_MODELS_CASCADE.length + 1,
          usedFallback: true,
          groqErrors: errors
        }
      };
    } catch (geminiError) {
      console.error(`${logPrefix} ‚ùå Gemini tamb√©m falhou:`, geminiError.message);
      errors.push({
        model: GEMINI_MODEL,
        provider: 'gemini',
        error: geminiError.message
      });
    }
  }
  
  const errorSummary = errors.map(e => `${e.model}: ${e.error}`).join(' | ');
  throw new Error(`Todos os modelos falharam ap√≥s m√∫ltiplas tentativas. Erros: ${errorSummary}`);
}

export async function generateWithCascade(messages, generationConfig = {}) {
  const {
    temperature = 0.7,
    max_tokens = 4000,
    top_p = 0.9
  } = generationConfig;

  return withMultiModelFallback(
    async (client, modelId) => {
      const response = await client.chat.completions.create({
        model: modelId,
        messages,
        temperature,
        max_tokens,
        top_p
      });
      return response;
    },
    {
      maxRetriesPerModel: 2,
      useGeminiFallback: true,
      logPrefix: '[GROQ-CASCADE]',
      messages,
      generationConfig: { temperature, max_tokens, top_p }
    }
  );
}

export async function withRetryAndTimeout(asyncFn, maxRetries = 3) {
  let lastError = null;
  
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await asyncFn();
    } catch (err) {
      lastError = err;
      
      if (err.status === 429 && attempt < maxRetries - 1) {
        const waitTime = Math.pow(2, attempt) * 1000;
        console.warn(`‚è≥ Rate limited. Waiting ${waitTime}ms...`);
        await new Promise(r => setTimeout(r, waitTime));
      } else if (err.code === 'ECONNREFUSED' && attempt < maxRetries - 1) {
        const waitTime = (attempt + 1) * 2000;
        console.warn(`‚è≥ Connection refused. Retrying in ${waitTime}ms...`);
        await new Promise(r => setTimeout(r, waitTime));
      } else if (attempt < maxRetries - 1) {
        const waitTime = Math.pow(2, attempt) * 1000;
        console.warn(`‚ö†Ô∏è Request failed: ${err.message}. Retrying in ${waitTime}ms...`);
        await new Promise(r => setTimeout(r, waitTime));
      } else {
        throw err;
      }
    }
  }
  
  throw lastError;
}

function parseJsonResponse(content) {
  if (!content || content.trim() === '') {
    console.warn('‚ö†Ô∏è Empty response from model');
    return null;
  }

  try {
    const jsonStr = content
      .replace(/```json\n?/g, '')
      .replace(/```\n?/g, '')
      .trim();
    
    const jsonMatch = jsonStr.match(/\[[\s\S]*\]|\{[\s\S]*\}/);
    if (jsonMatch) {
      return JSON.parse(jsonMatch[0]);
    }
    return JSON.parse(jsonStr);
  } catch (err) {
    console.error('‚ùå JSON parse error:', err.message);
    return null;
  }
}

export async function generateFlashcards(topic, quantity = 5) {
  const messages = [
    {
      role: 'system',
      content: 'Voc√™ √© especialista em educa√ß√£o. RESPONDA APENAS EM JSON V√ÅLIDO. N√£o inclua markdown ou texto extra.'
    },
    {
      role: 'user',
      content: `Crie ${quantity} flashcards sobre "${topic}". Formato obrigat√≥rio: [{"pergunta": "...", "resposta": "..."}]`
    }
  ];

  const result = await generateWithCascade(messages, {
    temperature: 0.5,
    max_tokens: 2000,
    top_p: 0.9
  });

  const content = result.choices?.[0]?.message?.content;
  const parsed = parseJsonResponse(content);
  
  if (Array.isArray(parsed)) {
    return parsed;
  }
  
  console.warn('‚ö†Ô∏è Could not parse flashcards response, returning empty array');
  return [];
}

export async function generateQuiz(topic, questions = 5) {
  const messages = [
    {
      role: 'system',
      content: 'Voc√™ cria quizzes educativos. RESPONDA APENAS EM JSON V√ÅLIDO. N√£o inclua markdown ou texto extra.'
    },
    {
      role: 'user',
      content: `Gere ${questions} quest√µes de m√∫ltipla escolha sobre "${topic}". Formato obrigat√≥rio: [{"pergunta": "...", "opcoes": ["a) op√ß√£o1", "b) op√ß√£o2", "c) op√ß√£o3", "d) op√ß√£o4"], "resposta": "letra correta"}]`
    }
  ];

  const result = await generateWithCascade(messages, {
    temperature: 0.3,
    max_tokens: 2000,
    top_p: 0.9
  });

  const content = result.choices?.[0]?.message?.content;
  const parsed = parseJsonResponse(content);
  
  if (Array.isArray(parsed)) {
    return parsed;
  }
  
  console.warn('‚ö†Ô∏è Could not parse quiz response, returning empty array');
  return [];
}

export async function generateTest(topic, questions = 10, difficulty = 'm√©dio') {
  const messages = [
    {
      role: 'system',
      content: `Voc√™ √© professor especialista. Crie prova com n√≠vel de dificuldade: ${difficulty}. A prova deve ser bem estruturada e educativa.`
    },
    {
      role: 'user',
      content: `Crie uma prova completa com ${questions} quest√µes sobre "${topic}". Inclua:
1. Cabe√ßalho com t√≠tulo da prova e espa√ßo para nome do aluno
2. Quest√µes numeradas (misture m√∫ltipla escolha e dissertativas)
3. Gabarito no final da prova

Formate de maneira clara e profissional.`
    }
  ];

  const result = await generateWithCascade(messages, {
    temperature: 0.2,
    max_tokens: 3000,
    top_p: 0.9
  });

  const content = result.choices?.[0]?.message?.content;
  return content || 'Erro ao gerar teste.';
}

export async function chat(userMessage, conversationHistory = []) {
  const messages = [
    {
      role: 'system',
      content: `Voc√™ √© o Epictus IA, um assistente educacional inteligente e amig√°vel.
Suas respostas devem ser:
- Claras e did√°ticas
- Encorajadoras e positivas
- Bem estruturadas com markdown quando apropriado
- Adaptadas ao contexto educacional
Sempre comece suas respostas com "Eai" de forma natural e amig√°vel.`
    },
    ...conversationHistory,
    { role: 'user', content: userMessage }
  ];

  const result = await generateWithCascade(messages, {
    temperature: 0.7,
    max_tokens: 1024,
    top_p: 0.9
  });

  const content = result.choices?.[0]?.message?.content;
  return content || 'Desculpe, n√£o consegui processar sua mensagem.';
}

function extractNumber(text) {
  const match = text.match(/(\d+)/);
  return match ? parseInt(match[1], 10) : null;
}

function extractDifficulty(text) {
  const lower = text.toLowerCase();
  if (lower.includes('f√°cil') || lower.includes('facil') || lower.includes('b√°sico')) {
    return 'f√°cil';
  }
  if (lower.includes('dif√≠cil') || lower.includes('dificil') || lower.includes('avan√ßado')) {
    return 'dif√≠cil';
  }
  return 'm√©dio';
}

export async function processUserPrompt(userPrompt, activityType = null) {
  try {
    const promptLower = userPrompt.toLowerCase();
    
    let detectedType = activityType || 'chat';
    
    if (!activityType) {
      if (promptLower.includes('flashcard') || promptLower.includes('cart√£o') || promptLower.includes('cart√µes')) {
        detectedType = 'flashcards';
      } else if (promptLower.includes('quiz') || promptLower.includes('quest√µes') || promptLower.includes('perguntas')) {
        detectedType = 'quiz';
      } else if (promptLower.includes('prova') || promptLower.includes('teste') || promptLower.includes('avalia√ß√£o')) {
        detectedType = 'test';
      }
    }
    
    let result;
    
    switch (detectedType) {
      case 'flashcards':
        const flashcardCount = extractNumber(userPrompt) || 5;
        result = await generateFlashcards(userPrompt, flashcardCount);
        break;
        
      case 'quiz':
        const quizCount = extractNumber(userPrompt) || 5;
        result = await generateQuiz(userPrompt, quizCount);
        break;
        
      case 'test':
        const testCount = extractNumber(userPrompt) || 10;
        const difficulty = extractDifficulty(userPrompt);
        result = await generateTest(userPrompt, testCount, difficulty);
        break;
        
      default:
        result = await chat(userPrompt, []);
        break;
    }
    
    return {
      type: detectedType,
      data: result,
      success: true
    };
  } catch (err) {
    console.error('Erro ao processar prompt:', err);
    return {
      type: activityType || 'chat',
      data: '',
      success: false,
      error: err.message || 'Erro desconhecido ao processar prompt'
    };
  }
}

export async function testGroqConnection() {
  try {
    const client = getGroqClient();
    
    const response = await client.chat.completions.create({
      model: 'llama-3.3-70b-versatile',
      messages: [{ role: 'user', content: 'Responda apenas com "OK"' }],
      max_tokens: 10,
      temperature: 0
    });
    
    if (response.choices?.[0]?.message?.content) {
      console.log('‚úÖ Conex√£o com Groq: OK');
      return { success: true, message: '‚úÖ Conex√£o com Groq: OK' };
    }
    
    return { success: false, message: '‚ùå Resposta vazia do modelo' };
  } catch (err) {
    console.error('‚ùå Erro de conex√£o com Groq:', err.message);
    return { success: false, message: `‚ùå Erro: ${err.message}` };
  }
}

export { GROQ_MODELS_CASCADE, GEMINI_MODEL };

export default {
  generateFlashcards,
  generateQuiz,
  generateTest,
  chat,
  processUserPrompt,
  testGroqConnection,
  generateWithCascade,
  withMultiModelFallback,
  getGroqClient,
  withRetryAndTimeout,
  GROQ_MODELS_CASCADE,
  GEMINI_MODEL
};
